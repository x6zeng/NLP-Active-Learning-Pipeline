{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e04c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4226012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = pd.read_csv('../data/tweet_data_2022.csv')\n",
    "df_2023 = pd.read_csv('../data/tweet_data_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6665c20",
   "metadata": {},
   "source": [
    "### Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f72535",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_number = 3\n",
    "random_state = 42\n",
    "categories = ['SentimentScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09f8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_sent(sent):\n",
    "    if ((sent == 0) | (sent == 1) | (sent == 2)):\n",
    "        return 'Negative'\n",
    "    elif (sent == 3):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3034b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if type(text) == np.float:\n",
    "        return \"\"\n",
    "    temp = text.lower() # to lower case\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp) # remove @s\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp) # remove hashtags\n",
    "    temp = re.sub(r'http\\S+', '', temp) # remove links\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp) # remove links\n",
    "    temp = re.sub(r'\\n|[^a-zA-Z]', ' ', temp) # remove punctuation\n",
    "    temp = temp.replace(\"\\n\", \" \").split()\n",
    "    temp = [w for w in temp if not w in stopwords_] # remove stopwords\n",
    "    temp = [w for w in temp if not w.isdigit()] # remove numbers\n",
    "    temp = [unidecode.unidecode(w) for w in temp] # turn non-enlish letters to english letters\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1a4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(df, ratio, time):\n",
    "    #partiton\n",
    "    if time:\n",
    "        df.sort_values(by=['date'], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    df_rows = df.shape[0]\n",
    "    seed_num = math.floor(df_rows * ratio[0])\n",
    "    seed = df[:seed_num]\n",
    "    unlabeled_num = seed_num + (math.floor(df_rows * ratio[1]))\n",
    "    unlabeled = df[seed_num:unlabeled_num]\n",
    "    test = df[unlabeled_num:]\n",
    "    return seed, unlabeled, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea1f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(seed, model_index):\n",
    "    cv = 5\n",
    "    train, test = train_test_split(seed, random_state=random_state, test_size=0.2, shuffle=True)\n",
    "    X_train, X_test, Y_train, Y_test = train[['text_cleaned']], test[['text_cleaned']], train[['SentimentScore']], test[['SentimentScore']]\n",
    "    #Wrap in ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tf\", CountVectorizer(stop_words=stopwords_), 'text_cleaned'),\n",
    "            (\"tfidf\", TfidfVectorizer(stop_words=stopwords_), 'text_cleaned')]\n",
    "    )\n",
    "    #Define the model\n",
    "    model_lst = [\n",
    "#                  SVC(kernel='rbf', probability=True, random_state=random_state), #SVC RBF\n",
    "#                  SVC(kernel='poly', probability=True, degree=4, random_state=random_state), #SVC Poly\n",
    "                 MultinomialNB(fit_prior=True, class_prior=None), #Multinomial Naive Bayes\n",
    "                 RandomForestClassifier(random_state=random_state), #Random Forest\n",
    "                 LogisticRegression(solver='sag', random_state=random_state), #Logistic Regression (Ridge)\n",
    "                 LogisticRegression(C=1, penalty='l1', solver='liblinear', random_state=random_state), #Logistic Regression (Lasso)\n",
    "            ]\n",
    "    model = model_lst[model_index]\n",
    "    \n",
    "    #Build the pipeline\n",
    "    pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('clf', OneVsRestClassifier(model, n_jobs=1)),\n",
    "            ])\n",
    "    #Train the model\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    # compute the testing accuracy\n",
    "    prediction = pipeline.predict(pd.DataFrame(X_test))\n",
    "    #Cross Validation\n",
    "    val_score = round(np.mean(cross_val_score(pipeline, X_test, Y_test, cv=cv)), round_number)\n",
    "    return pipeline, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c16bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(x):\n",
    "    entropy = 0\n",
    "    for i in x:\n",
    "        entropy += i * math.log2(1/i)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73966b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_unlabeled(pipeline, unlabeled):\n",
    "    unlabeled_x = unlabeled[['text_cleaned']]\n",
    "    unlabeled_y = unlabeled[['SentimentScore']]\n",
    "    prob = pipeline.predict_proba(unlabeled_x)\n",
    "    unlabeled['prob'] = list(prob)\n",
    "    unlabeled['entropy'] = unlabeled['prob'].apply(calc_entropy)\n",
    "    unlabeled.sort_values(by=['entropy'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ba605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(pipeline, seed, unlabeled, instances, model_index):\n",
    "    # Sort the unlabeled data based on informativeness level\n",
    "    choose_unlabeled(pipeline, unlabeled)\n",
    "    # Update the unlabeled data and the info_data\n",
    "    info_data, unlabeled = unlabeled.iloc[:instances], unlabeled.iloc[instances:]\n",
    "    # Add selected data to the training set\n",
    "    seed = pd.concat([seed, info_data[['date', 'text', 'SentimentScore', 'text_cleaned']]])\n",
    "    pipeline, validation_score = train_model(seed, model_index)\n",
    "    return pipeline, validation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9df57",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e067efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stopwords = stopwords.words('english')\n",
    "additional_stopwords = ['none']\n",
    "original_stopwords.extend(additional_stopwords)\n",
    "stopwords_ = set(original_stopwords)\n",
    "\n",
    "#Selects only the tweets about China\n",
    "df = df_2022[df_2022['country']=='China']\n",
    "df = df[['date', 'text', 'id', 'Bucket', 'SentimentScore']]\n",
    "\n",
    "#Shuffle the data\n",
    "df = df.sample(frac=1, replace=False, random_state=1) \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Step 1: Remove tweets that do not have sentiment score\n",
    "#Step 2: Average the sentiment score for each unique tweet\n",
    "df = df.copy()[['date', 'text', 'id', 'SentimentScore']]\n",
    "df.dropna(subset=['SentimentScore'], inplace=True)\n",
    "\n",
    "df = pd.DataFrame(df.groupby(['date', 'text', 'id'])['SentimentScore'].mean())\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#Remove ambiguous labels\n",
    "range_lst = [0, 1, 2, 3, 4, 5]\n",
    "df = df[df['SentimentScore'].apply(lambda x: True if x in range_lst else False)]\n",
    "df['SentimentScore'] = df['SentimentScore'].apply(standardize_sent)\n",
    "\n",
    "#Remove duplicates\n",
    "df = df.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "df = df[['date', 'text', 'SentimentScore']]\n",
    "df[\"text_cleaned\"] = [clean_text(t) for t in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658847b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(df, model_name, model_index, training_method, balance, sampling_size, sort_by_time, partition_ratio):\n",
    "    output = {}\n",
    "    output['model_name'] = model_name\n",
    "    output['training_method'] = training_method\n",
    "    output['balance'] = balance\n",
    "    output['sampling_size'] = sampling_size\n",
    "    output['sort_by_time'] = sort_by_time\n",
    "    output['partition_ratio'] = partition_ratio\n",
    "        \n",
    "    # 1. Balance dataset \n",
    "    df_1, df_2, df_3 = df[df.SentimentScore=='Negative'], df[df.SentimentScore=='Neutral'], df[df.SentimentScore=='Positive']\n",
    "    df_lst = [df_1, df_2, df_3]\n",
    "    \n",
    "    # 1.1 Balance the label distribution  (33% Negative vs. 33% Neutral vs. 33% Positive)\n",
    "    if balance:\n",
    "        sample_size = min(df_1.shape[0], df_2.shape[0], df_3.shape[0])\n",
    "        if df_1.shape[0] > sample_size:\n",
    "            df_1 = resample(df_1, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "        if df_2.shape[0] > sample_size:\n",
    "            df_2 = resample(df_2, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "        if df_3.shape[0] > sample_size:\n",
    "            df_3 = resample(df_3, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "\n",
    "    # 1.2 Keep the natural label distribution\n",
    "    seed_1, unlabeled_1, test_1 = partition_data(df_1, partition_ratio, sort_by_time)\n",
    "    seed_2, unlabeled_2, test_2 = partition_data(df_2, partition_ratio, sort_by_time)\n",
    "    seed_3, unlabeled_3, test_3 = partition_data(df_3, partition_ratio, sort_by_time)\n",
    "    seed, unlabeled, test = pd.concat([seed_1, seed_2, seed_3]), pd.concat([unlabeled_1, unlabeled_2, unlabeled_3]), pd.concat([test_1, test_2, test_3])\n",
    "    output['seed_size'], output['unlabeled_size'], output['test_size'] = seed.shape[0], unlabeled.shape[0], test.shape[0]\n",
    "    \n",
    "    initial_seed = seed.copy()\n",
    "    initial_unlabeled = unlabeled.copy()\n",
    "    \n",
    "    # 2. Train the model\n",
    "    initial_pipeline, initial_val_score = train_model(initial_seed, model_index)\n",
    "    \n",
    "    # 3. Active Learning\n",
    "    if sampling_size == 0:\n",
    "        pipeline, val_score = initial_pipeline, initial_val_score\n",
    "        \n",
    "    # 3.1 Initial Model + Random Sampling\n",
    "    elif training_method == 'random_sampling':\n",
    "        if initial_unlabeled.shape[0] >= sampling_size:\n",
    "            sample_unlabeled = initial_unlabeled.sample(n=sampling_size, replace=False, random_state=random_state)\n",
    "        else:\n",
    "            sample_unlabeled = initial_unlabeled.sample(n=sampling_size, replace=True, random_state=random_state)\n",
    "        seed_and_sample_unlabeled_df = pd.concat([initial_seed, sample_unlabeled])\n",
    "        pipeline, val_score = train_model(seed_and_sample_unlabeled_df, model_index)\n",
    "        \n",
    "    # 3.2 Initial Model + Active Learning\n",
    "    else:\n",
    "        pipeline, val_score = active_learning(initial_pipeline, initial_seed, initial_unlabeled, sampling_size, model_index)\n",
    "\n",
    "    # 4. Report Model Accuracy\n",
    "    X_test, Y_test = test[['text_cleaned']], test[['SentimentScore']]\n",
    "    prediction = pipeline.predict(pd.DataFrame(X_test))\n",
    "    accuracy = round(accuracy_score(Y_test, prediction), round_number)\n",
    "    \n",
    "    f1_micro = round(f1_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "    f1_macro = round(f1_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "    f1_weighted = round(f1_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "    \n",
    "    precision_micro = round(precision_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "    precision_macro = round(precision_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "    precision_weighted = round(precision_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "        \n",
    "    recall_micro = round(recall_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "    recall_macro = round(recall_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "    recall_weighted = round(recall_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "    \n",
    "    output['val_score'] = val_score\n",
    "    output['accuracy'] = accuracy\n",
    "    output['f1_micro'], output['f1_macro'], output['f1_weighted'] = f1_micro, f1_macro, f1_weighted\n",
    "    output['precision_micro'], output['precision_macro'], output['precision_weighted'] = precision_micro, precision_macro, precision_weighted\n",
    "    output['recall_micro'], output['recall_macro'], output['recall_weighted'] = recall_micro, recall_macro, recall_weighted\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e32509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = ['SVC RBF', 'SVC Poly', 'Bernoulli Naive Bayes', 'Random Forest Classifier', \n",
    "#               'Logistic Regression', 'Lasso', 'Ridge']\n",
    "model_name = ['Multinomial Naive Bayes', 'Random Forest Classifier', 'Logistic Regression (Ridge)', 'Logistic Regression (Lasso)']\n",
    "training_method = ['random_sampling', 'active_learning']\n",
    "balanced = [True, False]\n",
    "sampling_size = [0, 100, 200, 300, 400, 500, 600]\n",
    "sort_by_time = [True, False]\n",
    "partition_ratio = [[0.2, 0.4, 0.4], [0.5, 0.25, 0.25], [0.8, 0.1, 0.1]]\n",
    "\n",
    "\n",
    "# model_name = ['Multinomial Naive Bayes', 'Random Forest Classifier', 'Logistic Regression (Ridge)', 'Logistic Regression (Lasso)']\n",
    "# training_method = ['active_learning']\n",
    "# balanced = [True]\n",
    "# sampling_size = [10]\n",
    "# sort_by_time = [False]\n",
    "# partition_ratio = [[0.8, 0.1, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ec5589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Multinomial Naive Bayes\n",
      "2\n",
      "Multinomial Naive Bayes\n",
      "3\n",
      "Multinomial Naive Bayes\n",
      "4\n",
      "Multinomial Naive Bayes\n",
      "5\n",
      "Multinomial Naive Bayes\n",
      "6\n",
      "Multinomial Naive Bayes\n",
      "7\n",
      "Multinomial Naive Bayes\n",
      "8\n",
      "Multinomial Naive Bayes\n",
      "9\n",
      "Multinomial Naive Bayes\n",
      "10\n",
      "Multinomial Naive Bayes\n",
      "11\n",
      "Multinomial Naive Bayes\n",
      "12\n",
      "Multinomial Naive Bayes\n",
      "13\n",
      "Multinomial Naive Bayes\n",
      "14\n",
      "Multinomial Naive Bayes\n",
      "15\n",
      "Multinomial Naive Bayes\n",
      "16\n",
      "Multinomial Naive Bayes\n",
      "17\n",
      "Multinomial Naive Bayes\n",
      "18\n",
      "Multinomial Naive Bayes\n",
      "19\n",
      "Multinomial Naive Bayes\n",
      "20\n",
      "Multinomial Naive Bayes\n",
      "21\n",
      "Multinomial Naive Bayes\n",
      "22\n",
      "Multinomial Naive Bayes\n",
      "23\n",
      "Multinomial Naive Bayes\n",
      "24\n",
      "Multinomial Naive Bayes\n",
      "25\n",
      "Multinomial Naive Bayes\n",
      "26\n",
      "Multinomial Naive Bayes\n",
      "27\n",
      "Multinomial Naive Bayes\n",
      "28\n",
      "Multinomial Naive Bayes\n",
      "29\n",
      "Multinomial Naive Bayes\n",
      "30\n",
      "Multinomial Naive Bayes\n",
      "31\n",
      "Multinomial Naive Bayes\n",
      "32\n",
      "Multinomial Naive Bayes\n",
      "33\n",
      "Multinomial Naive Bayes\n",
      "34\n",
      "Multinomial Naive Bayes\n",
      "35\n",
      "Multinomial Naive Bayes\n",
      "36\n",
      "Multinomial Naive Bayes\n",
      "37\n",
      "Multinomial Naive Bayes\n",
      "38\n",
      "Multinomial Naive Bayes\n",
      "39\n",
      "Multinomial Naive Bayes\n",
      "40\n",
      "Multinomial Naive Bayes\n",
      "41\n",
      "Multinomial Naive Bayes\n",
      "42\n",
      "Multinomial Naive Bayes\n",
      "43\n",
      "Multinomial Naive Bayes\n",
      "44\n",
      "Multinomial Naive Bayes\n",
      "45\n",
      "Multinomial Naive Bayes\n",
      "46\n",
      "Multinomial Naive Bayes\n",
      "47\n",
      "Multinomial Naive Bayes\n",
      "48\n",
      "Multinomial Naive Bayes\n",
      "49\n",
      "Multinomial Naive Bayes\n",
      "50\n",
      "Multinomial Naive Bayes\n",
      "51\n",
      "Multinomial Naive Bayes\n",
      "52\n",
      "Multinomial Naive Bayes\n",
      "53\n",
      "Multinomial Naive Bayes\n",
      "54\n",
      "Multinomial Naive Bayes\n",
      "55\n",
      "Multinomial Naive Bayes\n",
      "56\n",
      "Multinomial Naive Bayes\n",
      "57\n",
      "Multinomial Naive Bayes\n",
      "58\n",
      "Multinomial Naive Bayes\n",
      "59\n",
      "Multinomial Naive Bayes\n",
      "60\n",
      "Multinomial Naive Bayes\n",
      "61\n",
      "Multinomial Naive Bayes\n",
      "62\n",
      "Multinomial Naive Bayes\n",
      "63\n",
      "Multinomial Naive Bayes\n",
      "64\n",
      "Multinomial Naive Bayes\n",
      "65\n",
      "Multinomial Naive Bayes\n",
      "66\n",
      "Multinomial Naive Bayes\n",
      "67\n",
      "Multinomial Naive Bayes\n",
      "68\n",
      "Multinomial Naive Bayes\n",
      "69\n",
      "Multinomial Naive Bayes\n",
      "70\n",
      "Multinomial Naive Bayes\n",
      "71\n",
      "Multinomial Naive Bayes\n",
      "72\n",
      "Multinomial Naive Bayes\n",
      "73\n",
      "Multinomial Naive Bayes\n",
      "74\n",
      "Multinomial Naive Bayes\n",
      "75\n",
      "Multinomial Naive Bayes\n",
      "76\n",
      "Multinomial Naive Bayes\n",
      "77\n",
      "Multinomial Naive Bayes\n",
      "78\n",
      "Multinomial Naive Bayes\n",
      "79\n",
      "Multinomial Naive Bayes\n",
      "80\n",
      "Multinomial Naive Bayes\n",
      "81\n",
      "Multinomial Naive Bayes\n",
      "82\n",
      "Multinomial Naive Bayes\n",
      "83\n",
      "Multinomial Naive Bayes\n",
      "84\n",
      "Multinomial Naive Bayes\n",
      "85\n",
      "Multinomial Naive Bayes\n",
      "86\n",
      "Multinomial Naive Bayes\n",
      "87\n",
      "Multinomial Naive Bayes\n",
      "88\n",
      "Multinomial Naive Bayes\n",
      "89\n",
      "Multinomial Naive Bayes\n",
      "90\n",
      "Multinomial Naive Bayes\n",
      "91\n",
      "Multinomial Naive Bayes\n",
      "92\n",
      "Multinomial Naive Bayes\n",
      "93\n",
      "Multinomial Naive Bayes\n",
      "94\n",
      "Multinomial Naive Bayes\n",
      "95\n",
      "Multinomial Naive Bayes\n",
      "96\n",
      "Multinomial Naive Bayes\n",
      "97\n",
      "Multinomial Naive Bayes\n",
      "98\n",
      "Multinomial Naive Bayes\n",
      "99\n",
      "Multinomial Naive Bayes\n",
      "100\n",
      "Multinomial Naive Bayes\n",
      "101\n",
      "Multinomial Naive Bayes\n",
      "102\n",
      "Multinomial Naive Bayes\n",
      "103\n",
      "Multinomial Naive Bayes\n",
      "104\n",
      "Multinomial Naive Bayes\n",
      "105\n",
      "Multinomial Naive Bayes\n",
      "106\n",
      "Multinomial Naive Bayes\n",
      "107\n",
      "Multinomial Naive Bayes\n",
      "108\n",
      "Multinomial Naive Bayes\n",
      "109\n",
      "Multinomial Naive Bayes\n",
      "110\n",
      "Multinomial Naive Bayes\n",
      "111\n",
      "Multinomial Naive Bayes\n",
      "112\n",
      "Multinomial Naive Bayes\n",
      "113\n",
      "Multinomial Naive Bayes\n",
      "114\n",
      "Multinomial Naive Bayes\n",
      "115\n",
      "Multinomial Naive Bayes\n",
      "116\n",
      "Multinomial Naive Bayes\n",
      "117\n",
      "Multinomial Naive Bayes\n",
      "118\n",
      "Multinomial Naive Bayes\n",
      "119\n",
      "Multinomial Naive Bayes\n",
      "120\n",
      "Multinomial Naive Bayes\n",
      "121\n",
      "Multinomial Naive Bayes\n",
      "122\n",
      "Multinomial Naive Bayes\n",
      "123\n",
      "Multinomial Naive Bayes\n",
      "124\n",
      "Multinomial Naive Bayes\n",
      "125\n",
      "Multinomial Naive Bayes\n",
      "126\n",
      "Multinomial Naive Bayes\n",
      "127\n",
      "Multinomial Naive Bayes\n",
      "128\n",
      "Multinomial Naive Bayes\n",
      "129\n",
      "Multinomial Naive Bayes\n",
      "130\n",
      "Multinomial Naive Bayes\n",
      "131\n",
      "Multinomial Naive Bayes\n",
      "132\n",
      "Multinomial Naive Bayes\n",
      "133\n",
      "Multinomial Naive Bayes\n",
      "134\n",
      "Multinomial Naive Bayes\n",
      "135\n",
      "Multinomial Naive Bayes\n",
      "136\n",
      "Multinomial Naive Bayes\n",
      "137\n",
      "Multinomial Naive Bayes\n",
      "138\n",
      "Multinomial Naive Bayes\n",
      "139\n",
      "Multinomial Naive Bayes\n",
      "140\n",
      "Multinomial Naive Bayes\n",
      "141\n",
      "Multinomial Naive Bayes\n",
      "142\n",
      "Multinomial Naive Bayes\n",
      "143\n",
      "Multinomial Naive Bayes\n",
      "144\n",
      "Multinomial Naive Bayes\n",
      "145\n",
      "Random Forest Classifier\n",
      "146\n",
      "Random Forest Classifier\n",
      "147\n",
      "Random Forest Classifier\n",
      "148\n",
      "Random Forest Classifier\n",
      "149\n",
      "Random Forest Classifier\n",
      "150\n",
      "Random Forest Classifier\n",
      "151\n",
      "Random Forest Classifier\n",
      "152\n",
      "Random Forest Classifier\n",
      "153\n",
      "Random Forest Classifier\n",
      "154\n",
      "Random Forest Classifier\n",
      "155\n",
      "Random Forest Classifier\n",
      "156\n",
      "Random Forest Classifier\n",
      "157\n",
      "Random Forest Classifier\n",
      "158\n",
      "Random Forest Classifier\n",
      "159\n",
      "Random Forest Classifier\n",
      "160\n",
      "Random Forest Classifier\n",
      "161\n",
      "Random Forest Classifier\n",
      "162\n",
      "Random Forest Classifier\n",
      "163\n",
      "Random Forest Classifier\n",
      "164\n",
      "Random Forest Classifier\n",
      "165\n",
      "Random Forest Classifier\n",
      "166\n",
      "Random Forest Classifier\n",
      "167\n",
      "Random Forest Classifier\n",
      "168\n",
      "Random Forest Classifier\n",
      "169\n",
      "Random Forest Classifier\n",
      "170\n",
      "Random Forest Classifier\n",
      "171\n",
      "Random Forest Classifier\n",
      "172\n",
      "Random Forest Classifier\n",
      "173\n",
      "Random Forest Classifier\n",
      "174\n",
      "Random Forest Classifier\n",
      "175\n",
      "Random Forest Classifier\n",
      "176\n",
      "Random Forest Classifier\n",
      "177\n",
      "Random Forest Classifier\n",
      "178\n",
      "Random Forest Classifier\n",
      "179\n",
      "Random Forest Classifier\n",
      "180\n",
      "Random Forest Classifier\n",
      "181\n",
      "Random Forest Classifier\n",
      "182\n",
      "Random Forest Classifier\n",
      "183\n",
      "Random Forest Classifier\n",
      "184\n",
      "Random Forest Classifier\n",
      "185\n",
      "Random Forest Classifier\n",
      "186\n",
      "Random Forest Classifier\n",
      "187\n",
      "Random Forest Classifier\n",
      "188\n",
      "Random Forest Classifier\n",
      "189\n",
      "Random Forest Classifier\n",
      "190\n",
      "Random Forest Classifier\n",
      "191\n",
      "Random Forest Classifier\n",
      "192\n",
      "Random Forest Classifier\n",
      "193\n",
      "Random Forest Classifier\n",
      "194\n",
      "Random Forest Classifier\n",
      "195\n",
      "Random Forest Classifier\n",
      "196\n",
      "Random Forest Classifier\n",
      "197\n",
      "Random Forest Classifier\n",
      "198\n",
      "Random Forest Classifier\n",
      "199\n",
      "Random Forest Classifier\n",
      "200\n",
      "Random Forest Classifier\n",
      "201\n",
      "Random Forest Classifier\n",
      "202\n",
      "Random Forest Classifier\n",
      "203\n",
      "Random Forest Classifier\n",
      "204\n",
      "Random Forest Classifier\n",
      "205\n",
      "Random Forest Classifier\n",
      "206\n",
      "Random Forest Classifier\n",
      "207\n",
      "Random Forest Classifier\n",
      "208\n",
      "Random Forest Classifier\n",
      "209\n",
      "Random Forest Classifier\n",
      "210\n",
      "Random Forest Classifier\n",
      "211\n",
      "Random Forest Classifier\n",
      "212\n",
      "Random Forest Classifier\n",
      "213\n",
      "Random Forest Classifier\n",
      "214\n",
      "Random Forest Classifier\n",
      "215\n",
      "Random Forest Classifier\n",
      "216\n",
      "Random Forest Classifier\n",
      "217\n",
      "Random Forest Classifier\n",
      "218\n",
      "Random Forest Classifier\n",
      "219\n",
      "Random Forest Classifier\n",
      "220\n",
      "Random Forest Classifier\n",
      "221\n",
      "Random Forest Classifier\n",
      "222\n",
      "Random Forest Classifier\n",
      "223\n",
      "Random Forest Classifier\n",
      "224\n",
      "Random Forest Classifier\n",
      "225\n",
      "Random Forest Classifier\n",
      "226\n",
      "Random Forest Classifier\n",
      "227\n",
      "Random Forest Classifier\n",
      "228\n",
      "Random Forest Classifier\n",
      "229\n",
      "Random Forest Classifier\n",
      "230\n",
      "Random Forest Classifier\n",
      "231\n",
      "Random Forest Classifier\n",
      "232\n",
      "Random Forest Classifier\n",
      "233\n",
      "Random Forest Classifier\n",
      "234\n",
      "Random Forest Classifier\n",
      "235\n",
      "Random Forest Classifier\n",
      "236\n",
      "Random Forest Classifier\n",
      "237\n",
      "Random Forest Classifier\n",
      "238\n",
      "Random Forest Classifier\n",
      "239\n",
      "Random Forest Classifier\n",
      "240\n",
      "Random Forest Classifier\n",
      "241\n",
      "Random Forest Classifier\n",
      "242\n",
      "Random Forest Classifier\n",
      "243\n",
      "Random Forest Classifier\n",
      "244\n",
      "Random Forest Classifier\n",
      "245\n",
      "Random Forest Classifier\n",
      "246\n",
      "Random Forest Classifier\n",
      "247\n",
      "Random Forest Classifier\n",
      "248\n",
      "Random Forest Classifier\n",
      "249\n",
      "Random Forest Classifier\n",
      "250\n",
      "Random Forest Classifier\n",
      "251\n",
      "Random Forest Classifier\n",
      "252\n",
      "Random Forest Classifier\n",
      "253\n",
      "Random Forest Classifier\n",
      "254\n",
      "Random Forest Classifier\n",
      "255\n",
      "Random Forest Classifier\n",
      "256\n",
      "Random Forest Classifier\n",
      "257\n",
      "Random Forest Classifier\n",
      "258\n",
      "Random Forest Classifier\n",
      "259\n",
      "Random Forest Classifier\n",
      "260\n",
      "Random Forest Classifier\n",
      "261\n",
      "Random Forest Classifier\n",
      "262\n",
      "Random Forest Classifier\n",
      "263\n",
      "Random Forest Classifier\n",
      "264\n",
      "Random Forest Classifier\n",
      "265\n",
      "Random Forest Classifier\n",
      "266\n",
      "Random Forest Classifier\n",
      "267\n",
      "Random Forest Classifier\n",
      "268\n",
      "Random Forest Classifier\n",
      "269\n",
      "Random Forest Classifier\n",
      "270\n",
      "Random Forest Classifier\n",
      "271\n",
      "Random Forest Classifier\n",
      "272\n",
      "Random Forest Classifier\n",
      "273\n",
      "Random Forest Classifier\n",
      "274\n",
      "Random Forest Classifier\n",
      "275\n",
      "Random Forest Classifier\n",
      "276\n",
      "Random Forest Classifier\n",
      "277\n",
      "Random Forest Classifier\n",
      "278\n",
      "Random Forest Classifier\n",
      "279\n",
      "Random Forest Classifier\n",
      "280\n",
      "Random Forest Classifier\n",
      "281\n",
      "Random Forest Classifier\n",
      "282\n",
      "Random Forest Classifier\n",
      "283\n",
      "Random Forest Classifier\n",
      "284\n",
      "Random Forest Classifier\n",
      "285\n",
      "Random Forest Classifier\n",
      "286\n",
      "Random Forest Classifier\n",
      "287\n",
      "Random Forest Classifier\n",
      "288\n",
      "Random Forest Classifier\n",
      "289\n",
      "Logistic Regression (Ridge)\n",
      "290\n",
      "Logistic Regression (Ridge)\n",
      "291\n",
      "Logistic Regression (Ridge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292\n",
      "Logistic Regression (Ridge)\n",
      "293\n",
      "Logistic Regression (Ridge)\n",
      "294\n",
      "Logistic Regression (Ridge)\n",
      "295\n",
      "Logistic Regression (Ridge)\n",
      "296\n",
      "Logistic Regression (Ridge)\n",
      "297\n",
      "Logistic Regression (Ridge)\n",
      "298\n",
      "Logistic Regression (Ridge)\n",
      "299\n",
      "Logistic Regression (Ridge)\n",
      "300\n",
      "Logistic Regression (Ridge)\n",
      "301\n",
      "Logistic Regression (Ridge)\n",
      "302\n",
      "Logistic Regression (Ridge)\n",
      "303\n",
      "Logistic Regression (Ridge)\n",
      "304\n",
      "Logistic Regression (Ridge)\n",
      "305\n",
      "Logistic Regression (Ridge)\n",
      "306\n",
      "Logistic Regression (Ridge)\n",
      "307\n",
      "Logistic Regression (Ridge)\n",
      "308\n",
      "Logistic Regression (Ridge)\n",
      "309\n",
      "Logistic Regression (Ridge)\n",
      "310\n",
      "Logistic Regression (Ridge)\n",
      "311\n",
      "Logistic Regression (Ridge)\n",
      "312\n",
      "Logistic Regression (Ridge)\n",
      "313\n",
      "Logistic Regression (Ridge)\n",
      "314\n",
      "Logistic Regression (Ridge)\n",
      "315\n",
      "Logistic Regression (Ridge)\n",
      "316\n",
      "Logistic Regression (Ridge)\n",
      "317\n",
      "Logistic Regression (Ridge)\n",
      "318\n",
      "Logistic Regression (Ridge)\n",
      "319\n",
      "Logistic Regression (Ridge)\n",
      "320\n",
      "Logistic Regression (Ridge)\n",
      "321\n",
      "Logistic Regression (Ridge)\n",
      "322\n",
      "Logistic Regression (Ridge)\n",
      "323\n",
      "Logistic Regression (Ridge)\n",
      "324\n",
      "Logistic Regression (Ridge)\n",
      "325\n",
      "Logistic Regression (Ridge)\n",
      "326\n",
      "Logistic Regression (Ridge)\n",
      "327\n",
      "Logistic Regression (Ridge)\n",
      "328\n",
      "Logistic Regression (Ridge)\n",
      "329\n",
      "Logistic Regression (Ridge)\n",
      "330\n",
      "Logistic Regression (Ridge)\n",
      "331\n",
      "Logistic Regression (Ridge)\n",
      "332\n",
      "Logistic Regression (Ridge)\n",
      "333\n",
      "Logistic Regression (Ridge)\n",
      "334\n",
      "Logistic Regression (Ridge)\n",
      "335\n",
      "Logistic Regression (Ridge)\n",
      "336\n",
      "Logistic Regression (Ridge)\n",
      "337\n",
      "Logistic Regression (Ridge)\n",
      "338\n",
      "Logistic Regression (Ridge)\n",
      "339\n",
      "Logistic Regression (Ridge)\n",
      "340\n",
      "Logistic Regression (Ridge)\n",
      "341\n",
      "Logistic Regression (Ridge)\n",
      "342\n",
      "Logistic Regression (Ridge)\n",
      "343\n",
      "Logistic Regression (Ridge)\n",
      "344\n",
      "Logistic Regression (Ridge)\n",
      "345\n",
      "Logistic Regression (Ridge)\n",
      "346\n",
      "Logistic Regression (Ridge)\n",
      "347\n",
      "Logistic Regression (Ridge)\n",
      "348\n",
      "Logistic Regression (Ridge)\n",
      "349\n",
      "Logistic Regression (Ridge)\n",
      "350\n",
      "Logistic Regression (Ridge)\n",
      "351\n",
      "Logistic Regression (Ridge)\n",
      "352\n",
      "Logistic Regression (Ridge)\n",
      "353\n",
      "Logistic Regression (Ridge)\n",
      "354\n",
      "Logistic Regression (Ridge)\n",
      "355\n",
      "Logistic Regression (Ridge)\n",
      "356\n",
      "Logistic Regression (Ridge)\n",
      "357\n",
      "Logistic Regression (Ridge)\n",
      "358\n",
      "Logistic Regression (Ridge)\n",
      "359\n",
      "Logistic Regression (Ridge)\n",
      "360\n",
      "Logistic Regression (Ridge)\n",
      "361\n",
      "Logistic Regression (Ridge)\n",
      "362\n",
      "Logistic Regression (Ridge)\n",
      "363\n",
      "Logistic Regression (Ridge)\n",
      "364\n",
      "Logistic Regression (Ridge)\n",
      "365\n",
      "Logistic Regression (Ridge)\n",
      "366\n",
      "Logistic Regression (Ridge)\n",
      "367\n",
      "Logistic Regression (Ridge)\n",
      "368\n",
      "Logistic Regression (Ridge)\n",
      "369\n",
      "Logistic Regression (Ridge)\n",
      "370\n",
      "Logistic Regression (Ridge)\n",
      "371\n",
      "Logistic Regression (Ridge)\n",
      "372\n",
      "Logistic Regression (Ridge)\n",
      "373\n",
      "Logistic Regression (Ridge)\n",
      "374\n",
      "Logistic Regression (Ridge)\n",
      "375\n",
      "Logistic Regression (Ridge)\n",
      "376\n",
      "Logistic Regression (Ridge)\n",
      "377\n",
      "Logistic Regression (Ridge)\n",
      "378\n",
      "Logistic Regression (Ridge)\n",
      "379\n",
      "Logistic Regression (Ridge)\n",
      "380\n",
      "Logistic Regression (Ridge)\n",
      "381\n",
      "Logistic Regression (Ridge)\n",
      "382\n",
      "Logistic Regression (Ridge)\n",
      "383\n",
      "Logistic Regression (Ridge)\n",
      "384\n",
      "Logistic Regression (Ridge)\n",
      "385\n",
      "Logistic Regression (Ridge)\n",
      "386\n",
      "Logistic Regression (Ridge)\n",
      "387\n",
      "Logistic Regression (Ridge)\n",
      "388\n",
      "Logistic Regression (Ridge)\n",
      "389\n",
      "Logistic Regression (Ridge)\n",
      "390\n",
      "Logistic Regression (Ridge)\n",
      "391\n",
      "Logistic Regression (Ridge)\n",
      "392\n",
      "Logistic Regression (Ridge)\n",
      "393\n",
      "Logistic Regression (Ridge)\n",
      "394\n",
      "Logistic Regression (Ridge)\n",
      "395\n",
      "Logistic Regression (Ridge)\n",
      "396\n",
      "Logistic Regression (Ridge)\n",
      "397\n",
      "Logistic Regression (Ridge)\n",
      "398\n",
      "Logistic Regression (Ridge)\n",
      "399\n",
      "Logistic Regression (Ridge)\n",
      "400\n",
      "Logistic Regression (Ridge)\n",
      "401\n",
      "Logistic Regression (Ridge)\n",
      "402\n",
      "Logistic Regression (Ridge)\n",
      "403\n",
      "Logistic Regression (Ridge)\n",
      "404\n",
      "Logistic Regression (Ridge)\n",
      "405\n",
      "Logistic Regression (Ridge)\n",
      "406\n",
      "Logistic Regression (Ridge)\n",
      "407\n",
      "Logistic Regression (Ridge)\n",
      "408\n",
      "Logistic Regression (Ridge)\n",
      "409\n",
      "Logistic Regression (Ridge)\n",
      "410\n",
      "Logistic Regression (Ridge)\n",
      "411\n",
      "Logistic Regression (Ridge)\n",
      "412\n",
      "Logistic Regression (Ridge)\n",
      "413\n",
      "Logistic Regression (Ridge)\n",
      "414\n",
      "Logistic Regression (Ridge)\n",
      "415\n",
      "Logistic Regression (Ridge)\n",
      "416\n",
      "Logistic Regression (Ridge)\n",
      "417\n",
      "Logistic Regression (Ridge)\n",
      "418\n",
      "Logistic Regression (Ridge)\n",
      "419\n",
      "Logistic Regression (Ridge)\n",
      "420\n",
      "Logistic Regression (Ridge)\n",
      "421\n",
      "Logistic Regression (Ridge)\n",
      "422\n",
      "Logistic Regression (Ridge)\n",
      "423\n",
      "Logistic Regression (Ridge)\n",
      "424\n",
      "Logistic Regression (Ridge)\n",
      "425\n",
      "Logistic Regression (Ridge)\n",
      "426\n",
      "Logistic Regression (Ridge)\n",
      "427\n",
      "Logistic Regression (Ridge)\n",
      "428\n",
      "Logistic Regression (Ridge)\n",
      "429\n",
      "Logistic Regression (Ridge)\n",
      "430\n",
      "Logistic Regression (Ridge)\n",
      "431\n",
      "Logistic Regression (Ridge)\n",
      "432\n",
      "Logistic Regression (Ridge)\n",
      "433\n",
      "Logistic Regression (Lasso)\n",
      "434\n",
      "Logistic Regression (Lasso)\n",
      "435\n",
      "Logistic Regression (Lasso)\n",
      "436\n",
      "Logistic Regression (Lasso)\n",
      "437\n",
      "Logistic Regression (Lasso)\n",
      "438\n",
      "Logistic Regression (Lasso)\n",
      "439\n",
      "Logistic Regression (Lasso)\n",
      "440\n",
      "Logistic Regression (Lasso)\n",
      "441\n",
      "Logistic Regression (Lasso)\n",
      "442\n",
      "Logistic Regression (Lasso)\n",
      "443\n",
      "Logistic Regression (Lasso)\n",
      "444\n",
      "Logistic Regression (Lasso)\n",
      "445\n",
      "Logistic Regression (Lasso)\n",
      "446\n",
      "Logistic Regression (Lasso)\n",
      "447\n",
      "Logistic Regression (Lasso)\n",
      "448\n",
      "Logistic Regression (Lasso)\n",
      "449\n",
      "Logistic Regression (Lasso)\n",
      "450\n",
      "Logistic Regression (Lasso)\n",
      "451\n",
      "Logistic Regression (Lasso)\n",
      "452\n",
      "Logistic Regression (Lasso)\n",
      "453\n",
      "Logistic Regression (Lasso)\n",
      "454\n",
      "Logistic Regression (Lasso)\n",
      "455\n",
      "Logistic Regression (Lasso)\n",
      "456\n",
      "Logistic Regression (Lasso)\n",
      "457\n",
      "Logistic Regression (Lasso)\n",
      "458\n",
      "Logistic Regression (Lasso)\n",
      "459\n",
      "Logistic Regression (Lasso)\n",
      "460\n",
      "Logistic Regression (Lasso)\n",
      "461\n",
      "Logistic Regression (Lasso)\n",
      "462\n",
      "Logistic Regression (Lasso)\n",
      "463\n",
      "Logistic Regression (Lasso)\n",
      "464\n",
      "Logistic Regression (Lasso)\n",
      "465\n",
      "Logistic Regression (Lasso)\n",
      "466\n",
      "Logistic Regression (Lasso)\n",
      "467\n",
      "Logistic Regression (Lasso)\n",
      "468\n",
      "Logistic Regression (Lasso)\n",
      "469\n",
      "Logistic Regression (Lasso)\n",
      "470\n",
      "Logistic Regression (Lasso)\n",
      "471\n",
      "Logistic Regression (Lasso)\n",
      "472\n",
      "Logistic Regression (Lasso)\n",
      "473\n",
      "Logistic Regression (Lasso)\n",
      "474\n",
      "Logistic Regression (Lasso)\n",
      "475\n",
      "Logistic Regression (Lasso)\n",
      "476\n",
      "Logistic Regression (Lasso)\n",
      "477\n",
      "Logistic Regression (Lasso)\n",
      "478\n",
      "Logistic Regression (Lasso)\n",
      "479\n",
      "Logistic Regression (Lasso)\n",
      "480\n",
      "Logistic Regression (Lasso)\n",
      "481\n",
      "Logistic Regression (Lasso)\n",
      "482\n",
      "Logistic Regression (Lasso)\n",
      "483\n",
      "Logistic Regression (Lasso)\n",
      "484\n",
      "Logistic Regression (Lasso)\n",
      "485\n",
      "Logistic Regression (Lasso)\n",
      "486\n",
      "Logistic Regression (Lasso)\n",
      "487\n",
      "Logistic Regression (Lasso)\n",
      "488\n",
      "Logistic Regression (Lasso)\n",
      "489\n",
      "Logistic Regression (Lasso)\n",
      "490\n",
      "Logistic Regression (Lasso)\n",
      "491\n",
      "Logistic Regression (Lasso)\n",
      "492\n",
      "Logistic Regression (Lasso)\n",
      "493\n",
      "Logistic Regression (Lasso)\n",
      "494\n",
      "Logistic Regression (Lasso)\n",
      "495\n",
      "Logistic Regression (Lasso)\n",
      "496\n",
      "Logistic Regression (Lasso)\n",
      "497\n",
      "Logistic Regression (Lasso)\n",
      "498\n",
      "Logistic Regression (Lasso)\n",
      "499\n",
      "Logistic Regression (Lasso)\n",
      "500\n",
      "Logistic Regression (Lasso)\n",
      "501\n",
      "Logistic Regression (Lasso)\n",
      "502\n",
      "Logistic Regression (Lasso)\n",
      "503\n",
      "Logistic Regression (Lasso)\n",
      "504\n",
      "Logistic Regression (Lasso)\n",
      "505\n",
      "Logistic Regression (Lasso)\n",
      "506\n",
      "Logistic Regression (Lasso)\n",
      "507\n",
      "Logistic Regression (Lasso)\n",
      "508\n",
      "Logistic Regression (Lasso)\n",
      "509\n",
      "Logistic Regression (Lasso)\n",
      "510\n",
      "Logistic Regression (Lasso)\n",
      "511\n",
      "Logistic Regression (Lasso)\n",
      "512\n",
      "Logistic Regression (Lasso)\n",
      "513\n",
      "Logistic Regression (Lasso)\n",
      "514\n",
      "Logistic Regression (Lasso)\n",
      "515\n",
      "Logistic Regression (Lasso)\n",
      "516\n",
      "Logistic Regression (Lasso)\n",
      "517\n",
      "Logistic Regression (Lasso)\n",
      "518\n",
      "Logistic Regression (Lasso)\n",
      "519\n",
      "Logistic Regression (Lasso)\n",
      "520\n",
      "Logistic Regression (Lasso)\n",
      "521\n",
      "Logistic Regression (Lasso)\n",
      "522\n",
      "Logistic Regression (Lasso)\n",
      "523\n",
      "Logistic Regression (Lasso)\n",
      "524\n",
      "Logistic Regression (Lasso)\n",
      "525\n",
      "Logistic Regression (Lasso)\n",
      "526\n",
      "Logistic Regression (Lasso)\n",
      "527\n",
      "Logistic Regression (Lasso)\n",
      "528\n",
      "Logistic Regression (Lasso)\n",
      "529\n",
      "Logistic Regression (Lasso)\n",
      "530\n",
      "Logistic Regression (Lasso)\n",
      "531\n",
      "Logistic Regression (Lasso)\n",
      "532\n",
      "Logistic Regression (Lasso)\n",
      "533\n",
      "Logistic Regression (Lasso)\n",
      "534\n",
      "Logistic Regression (Lasso)\n",
      "535\n",
      "Logistic Regression (Lasso)\n",
      "536\n",
      "Logistic Regression (Lasso)\n",
      "537\n",
      "Logistic Regression (Lasso)\n",
      "538\n",
      "Logistic Regression (Lasso)\n",
      "539\n",
      "Logistic Regression (Lasso)\n",
      "540\n",
      "Logistic Regression (Lasso)\n",
      "541\n",
      "Logistic Regression (Lasso)\n",
      "542\n",
      "Logistic Regression (Lasso)\n",
      "543\n",
      "Logistic Regression (Lasso)\n",
      "544\n",
      "Logistic Regression (Lasso)\n",
      "545\n",
      "Logistic Regression (Lasso)\n",
      "546\n",
      "Logistic Regression (Lasso)\n",
      "547\n",
      "Logistic Regression (Lasso)\n",
      "548\n",
      "Logistic Regression (Lasso)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "Logistic Regression (Lasso)\n",
      "550\n",
      "Logistic Regression (Lasso)\n",
      "551\n",
      "Logistic Regression (Lasso)\n",
      "552\n",
      "Logistic Regression (Lasso)\n",
      "553\n",
      "Logistic Regression (Lasso)\n",
      "554\n",
      "Logistic Regression (Lasso)\n",
      "555\n",
      "Logistic Regression (Lasso)\n",
      "556\n",
      "Logistic Regression (Lasso)\n",
      "557\n",
      "Logistic Regression (Lasso)\n",
      "558\n",
      "Logistic Regression (Lasso)\n",
      "559\n",
      "Logistic Regression (Lasso)\n",
      "560\n",
      "Logistic Regression (Lasso)\n",
      "561\n",
      "Logistic Regression (Lasso)\n",
      "562\n",
      "Logistic Regression (Lasso)\n",
      "563\n",
      "Logistic Regression (Lasso)\n",
      "564\n",
      "Logistic Regression (Lasso)\n",
      "565\n",
      "Logistic Regression (Lasso)\n",
      "566\n",
      "Logistic Regression (Lasso)\n",
      "567\n",
      "Logistic Regression (Lasso)\n",
      "568\n",
      "Logistic Regression (Lasso)\n",
      "569\n",
      "Logistic Regression (Lasso)\n",
      "570\n",
      "Logistic Regression (Lasso)\n",
      "571\n",
      "Logistic Regression (Lasso)\n",
      "572\n",
      "Logistic Regression (Lasso)\n",
      "573\n",
      "Logistic Regression (Lasso)\n",
      "574\n",
      "Logistic Regression (Lasso)\n",
      "575\n",
      "Logistic Regression (Lasso)\n",
      "576\n",
      "Logistic Regression (Lasso)\n"
     ]
    }
   ],
   "source": [
    "model_result_df = pd.DataFrame()\n",
    "index = 1\n",
    "for mn in range(len(model_name)):\n",
    "    for tm in training_method:\n",
    "        for b in balanced:\n",
    "            for ss in sampling_size:\n",
    "                for t in sort_by_time:\n",
    "                    for r in partition_ratio:\n",
    "                        print(index)\n",
    "                        print(model_name[mn])\n",
    "                        model_output = trial(df, model_name[mn], mn, tm, b, ss, t, r)\n",
    "                        if index == 0:\n",
    "                            model_result_df = pd.DataFrame(model_output, index=index)\n",
    "                        else:\n",
    "                            model_result_df = model_result_df.append(pd.DataFrame([model_output],index=[index]))\n",
    "                        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "194a068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 20)\n"
     ]
    }
   ],
   "source": [
    "model_result_df.head()\n",
    "print(model_result_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f8a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_df.to_csv('sentiment_model_result.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a4a6beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>training_method</th>\n",
       "      <th>balance</th>\n",
       "      <th>sampling_size</th>\n",
       "      <th>sort_by_time</th>\n",
       "      <th>partition_ratio</th>\n",
       "      <th>seed_size</th>\n",
       "      <th>unlabeled_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>val_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>recall_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.2, 0.4, 0.4]</td>\n",
       "      <td>81</td>\n",
       "      <td>165</td>\n",
       "      <td>171</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>207</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.8, 0.1, 0.1]</td>\n",
       "      <td>333</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.2, 0.4, 0.4]</td>\n",
       "      <td>81</td>\n",
       "      <td>165</td>\n",
       "      <td>171</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>207</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>Logistic Regression (Lasso)</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>3682</td>\n",
       "      <td>1840</td>\n",
       "      <td>1843</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Logistic Regression (Lasso)</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.8, 0.1, 0.1]</td>\n",
       "      <td>5891</td>\n",
       "      <td>735</td>\n",
       "      <td>739</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Logistic Regression (Lasso)</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.2, 0.4, 0.4]</td>\n",
       "      <td>1471</td>\n",
       "      <td>2944</td>\n",
       "      <td>2950</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Logistic Regression (Lasso)</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>3682</td>\n",
       "      <td>1840</td>\n",
       "      <td>1843</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>Logistic Regression (Lasso)</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.8, 0.1, 0.1]</td>\n",
       "      <td>5891</td>\n",
       "      <td>735</td>\n",
       "      <td>739</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_name  training_method  balance  sampling_size  \\\n",
       "1        Multinomial Naive Bayes  random_sampling     True              0   \n",
       "2        Multinomial Naive Bayes  random_sampling     True              0   \n",
       "3        Multinomial Naive Bayes  random_sampling     True              0   \n",
       "4        Multinomial Naive Bayes  random_sampling     True              0   \n",
       "5        Multinomial Naive Bayes  random_sampling     True              0   \n",
       "..                           ...              ...      ...            ...   \n",
       "572  Logistic Regression (Lasso)  active_learning    False            500   \n",
       "573  Logistic Regression (Lasso)  active_learning    False            500   \n",
       "574  Logistic Regression (Lasso)  active_learning    False            500   \n",
       "575  Logistic Regression (Lasso)  active_learning    False            500   \n",
       "576  Logistic Regression (Lasso)  active_learning    False            500   \n",
       "\n",
       "     sort_by_time    partition_ratio  seed_size  unlabeled_size  test_size  \\\n",
       "1            True    [0.2, 0.4, 0.4]         81             165        171   \n",
       "2            True  [0.5, 0.25, 0.25]        207             102        108   \n",
       "3            True    [0.8, 0.1, 0.1]        333              39         45   \n",
       "4           False    [0.2, 0.4, 0.4]         81             165        171   \n",
       "5           False  [0.5, 0.25, 0.25]        207             102        108   \n",
       "..            ...                ...        ...             ...        ...   \n",
       "572          True  [0.5, 0.25, 0.25]       3682            1840       1843   \n",
       "573          True    [0.8, 0.1, 0.1]       5891             735        739   \n",
       "574         False    [0.2, 0.4, 0.4]       1471            2944       2950   \n",
       "575         False  [0.5, 0.25, 0.25]       3682            1840       1843   \n",
       "576         False    [0.8, 0.1, 0.1]       5891             735        739   \n",
       "\n",
       "     val_score  accuracy  f1_micro  f1_macro  f1_weighted  precision_micro  \\\n",
       "1        0.517     0.450     0.450     0.417        0.417            0.450   \n",
       "2        0.578     0.583     0.583     0.579        0.579            0.583   \n",
       "3        0.612     0.600     0.600     0.573        0.573            0.600   \n",
       "4        0.467     0.573     0.573     0.504        0.504            0.573   \n",
       "5        0.522     0.620     0.620     0.593        0.593            0.620   \n",
       "..         ...       ...       ...       ...          ...              ...   \n",
       "572      0.962     0.960     0.960     0.410        0.945            0.960   \n",
       "573      0.962     0.958     0.958     0.393        0.941            0.958   \n",
       "574      0.942     0.958     0.958     0.353        0.940            0.958   \n",
       "575      0.952     0.960     0.960     0.396        0.944            0.960   \n",
       "576      0.956     0.958     0.958     0.393        0.941            0.958   \n",
       "\n",
       "     precision_macro  precision_weighted  recall_micro  recall_macro  \\\n",
       "1              0.412               0.412         0.450         0.450   \n",
       "2              0.612               0.612         0.583         0.583   \n",
       "3              0.609               0.609         0.600         0.600   \n",
       "4              0.520               0.520         0.573         0.573   \n",
       "5              0.624               0.624         0.620         0.620   \n",
       "..               ...                 ...           ...           ...   \n",
       "572            0.654               0.944         0.960         0.383   \n",
       "573            0.542               0.933         0.958         0.372   \n",
       "574            0.445               0.928         0.958         0.348   \n",
       "575            0.559               0.938         0.960         0.374   \n",
       "576            0.542               0.933         0.958         0.372   \n",
       "\n",
       "     recall_weighted  \n",
       "1              0.450  \n",
       "2              0.583  \n",
       "3              0.600  \n",
       "4              0.573  \n",
       "5              0.620  \n",
       "..               ...  \n",
       "572            0.960  \n",
       "573            0.958  \n",
       "574            0.958  \n",
       "575            0.960  \n",
       "576            0.958  \n",
       "\n",
       "[576 rows x 20 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fed926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
