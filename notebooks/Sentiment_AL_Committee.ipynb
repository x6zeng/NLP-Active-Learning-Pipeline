{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e04c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import unidecode\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4226012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = pd.read_csv('../data/tweet_data_2022.csv')\n",
    "df_2023 = pd.read_csv('../data/tweet_data_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6665c20",
   "metadata": {},
   "source": [
    "### Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6b2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_number = 3\n",
    "random_state = 42\n",
    "categories = ['SentimentScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09f8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_sent(sent):\n",
    "    if ((sent == 0) | (sent == 1) | (sent == 2)):\n",
    "        return 'Negative'\n",
    "    elif (sent == 3):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3034b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if type(text) == np.float:\n",
    "        return \"\"\n",
    "    temp = text.lower() # to lower case\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp) # remove @s\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp) # remove hashtags\n",
    "    temp = re.sub(r'http\\S+', '', temp) # remove links\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp) # remove links\n",
    "    temp = re.sub(r'\\n|[^a-zA-Z]', ' ', temp) # remove punctuation\n",
    "    temp = temp.replace(\"\\n\", \" \").split()\n",
    "    temp = [w for w in temp if not w in stopwords_] # remove stopwords\n",
    "    temp = [w for w in temp if not w.isdigit()] # remove numbers\n",
    "    temp = [unidecode.unidecode(w) for w in temp] # turn non-enlish letters to english letters\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04826dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(df, ratio, time):\n",
    "    #partiton\n",
    "    if time:\n",
    "        df.sort_values(by=['date'], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    df_rows = df.shape[0]\n",
    "    seed_num = math.floor(df_rows * ratio[0])\n",
    "    seed = df[:seed_num]\n",
    "    unlabeled_num = seed_num + (math.floor(df_rows * ratio[1]))\n",
    "    unlabeled = df[seed_num:unlabeled_num]\n",
    "    test = df[unlabeled_num:]\n",
    "    return seed, unlabeled, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d091725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(seed):\n",
    "    cv = 5\n",
    "    train, test = train_test_split(seed, random_state=random_state, test_size=0.2, shuffle=True)\n",
    "    X_train, X_test, Y_train, Y_test = train[['text_cleaned']], test[['text_cleaned']], train[['SentimentScore']], test[['SentimentScore']]\n",
    "    #Wrap in ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tf\", CountVectorizer(stop_words=stopwords_), 'text_cleaned'),\n",
    "            (\"tfidf\", TfidfVectorizer(stop_words=stopwords_), 'text_cleaned')]\n",
    "    )\n",
    "    #Define the model\n",
    "    model_lst = [\n",
    "                SVC(),\n",
    "                KNeighborsClassifier(),\n",
    "                DecisionTreeClassifier(),\n",
    "                RandomForestClassifier(),\n",
    "                AdaBoostClassifier(),\n",
    "            ]\n",
    "\n",
    "    pl_preds = []\n",
    "    for model in model_lst:\n",
    "        #Build the pipeline\n",
    "        pipeline = Pipeline([\n",
    "                    ('preprocessor', preprocessor),\n",
    "                    ('clf', OneVsRestClassifier(model, n_jobs=1)),\n",
    "                ])\n",
    "        #Train the model\n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        # compute the testing accuracy\n",
    "        prediction = pipeline.predict(pd.DataFrame(X_test))\n",
    "        pl_preds.append([pipeline, prediction])\n",
    "        \n",
    "    #Saves all the model pipelines\n",
    "    pipelines = [x[0] for x in pl_preds]\n",
    "    #Saves all the model predictions\n",
    "    all_preds = np.array([x[1] for x in pl_preds]).transpose()\n",
    "    #Find the mode in all preds\n",
    "    final_preds = [mode(i) for i in all_preds]\n",
    "    accuracy = accuracy_score(Y_test,final_preds)\n",
    "    return pipelines, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836d90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(lst):\n",
    "    unique_num = list(set(lst))\n",
    "    entropy = 0\n",
    "    for i in range(len(unique_num)):\n",
    "        label = unique_num[i]\n",
    "        prob = sum(np.array(lst) == label)/len(lst)\n",
    "        entropy += prob * math.log2(1/prob)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39aef465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_unlabeled(pipelines, unlabeled):\n",
    "    unlabeled_x = unlabeled[['text_cleaned']]\n",
    "    unlabeled_y = unlabeled[['SentimentScore']]\n",
    "    all_preds = np.array([pl.predict(unlabeled_x) for pl in pipelines]).transpose()\n",
    "    unlabeled['all_preds'] = list(all_preds)\n",
    "    unlabeled['entropy'] = unlabeled['all_preds'].apply(calc_entropy)\n",
    "    unlabeled.sort_values(by=['entropy'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a29a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(pipelines, seed, unlabeled, instances):\n",
    "    # Sort the unlabeled data based on informativeness level\n",
    "    choose_unlabeled(pipelines, unlabeled)\n",
    "    # Update the unlabeled data and the info_data\n",
    "    info_data, unlabeled = unlabeled.iloc[:instances], unlabeled.iloc[instances:]\n",
    "    # Add selected data to the training set\n",
    "    seed = pd.concat([seed, info_data[['date', 'text', 'SentimentScore', 'text_cleaned']]])\n",
    "    pipelines, accuracy = train_model(seed)\n",
    "    return pipelines, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fc5bf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca671fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stopwords = stopwords.words('english')\n",
    "additional_stopwords = ['none']\n",
    "original_stopwords.extend(additional_stopwords)\n",
    "stopwords_ = set(original_stopwords)\n",
    "\n",
    "#Selects only the tweets about China\n",
    "df = df_2022[df_2022['country']=='China']\n",
    "df = df[['date', 'text', 'id', 'Bucket', 'SentimentScore']]\n",
    "\n",
    "#Shuffle the data\n",
    "df = df.sample(frac=1, replace=False, random_state=1) \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Step 1: Remove tweets that do not have sentiment score\n",
    "#Step 2: Average the sentiment score for each unique tweet\n",
    "df = df.copy()[['date', 'text', 'id', 'SentimentScore']]\n",
    "df.dropna(subset=['SentimentScore'], inplace=True)\n",
    "\n",
    "df = pd.DataFrame(df.groupby(['date', 'text', 'id'])['SentimentScore'].mean())\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#Remove ambiguous labels\n",
    "range_lst = [0, 1, 2, 3, 4, 5]\n",
    "df = df[df['SentimentScore'].apply(lambda x: True if x in range_lst else False)]\n",
    "df['SentimentScore'] = df['SentimentScore'].apply(standardize_sent)\n",
    "\n",
    "#Remove duplicates\n",
    "df = df.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "df = df[['date', 'text', 'SentimentScore']]\n",
    "df[\"text_cleaned\"] = [clean_text(t) for t in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7f3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(df, model_names, training_method, balance, sampling_size, sort_by_time, partition_ratio):\n",
    "    output = {}\n",
    "    output['model_names'] = model_names\n",
    "    output['training_method'] = training_method #random_sampling, active_learning\n",
    "    output['balance'] = balance\n",
    "    output['sampling_size'] = sampling_size\n",
    "    output['sort_by_time'] = sort_by_time\n",
    "    output['partition_ratio'] = partition_ratio\n",
    "    accuracy_lst = []\n",
    "    f1_micro_lst, f1_macro_lst, f1_weighted_lst = [], [], []\n",
    "    precision_micro_lst, precision_macro_lst, precision_weighted_lst = [], [], []\n",
    "    recall_micro_lst, recall_macro_lst, recall_weighted_lst = [], [], []    \n",
    "\n",
    "    for i in range(5):\n",
    "        # 1. Balance dataset \n",
    "        df_1, df_2, df_3 = df[df.SentimentScore=='Negative'], df[df.SentimentScore=='Neutral'], df[df.SentimentScore=='Positive']\n",
    "\n",
    "        # 1.1 Balance the label distribution  (33% Negative vs. 33% Neutral vs. 33% Positive)\n",
    "        if balance:\n",
    "            sample_size = min(df_1.shape[0], df_2.shape[0], df_3.shape[0])\n",
    "            if df_1.shape[0] > sample_size:\n",
    "                df_1 = resample(df_1, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "            if df_2.shape[0] > sample_size:\n",
    "                df_2 = resample(df_2, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "            if df_3.shape[0] > sample_size:\n",
    "                df_3 = resample(df_3, replace=False, n_samples=sample_size, random_state=random_state)\n",
    "\n",
    "        # 1.2 Keep the natural label distribution\n",
    "        seed_1, unlabeled_1, test_1 = partition_data(df_1, partition_ratio, sort_by_time)\n",
    "        seed_2, unlabeled_2, test_2 = partition_data(df_2, partition_ratio, sort_by_time)\n",
    "        seed_3, unlabeled_3, test_3 = partition_data(df_3, partition_ratio, sort_by_time)\n",
    "        seed, unlabeled, test = pd.concat([seed_1, seed_2, seed_3]), pd.concat([unlabeled_1, unlabeled_2, unlabeled_3]), pd.concat([test_1, test_2, test_3])\n",
    "        output['seed_size'], output['unlabeled_size'], output['test_size'] = seed.shape[0], unlabeled.shape[0], test.shape[0]\n",
    "\n",
    "        initial_seed = seed.copy()\n",
    "        initial_unlabeled = unlabeled.copy()\n",
    "\n",
    "        # 2. Train the model\n",
    "        initial_pipelines, initial_accuracy = train_model(initial_seed)\n",
    "\n",
    "        # 3. Active Learning\n",
    "        if sampling_size == 0:\n",
    "            pipelines, accuracy = initial_pipelines, initial_accuracy\n",
    "\n",
    "        # 3.1 Initial Model + Random Sampling\n",
    "        elif training_method == 'random_sampling':\n",
    "            if initial_unlabeled.shape[0] >= sampling_size:\n",
    "                sample_unlabeled = initial_unlabeled.sample(n=sampling_size, replace=False, random_state=i)\n",
    "            else:\n",
    "                sample_unlabeled = initial_unlabeled.sample(n=sampling_size, replace=True, random_state=i)\n",
    "            seed_and_sample_unlabeled_df = pd.concat([initial_seed, sample_unlabeled])\n",
    "            pipelines, accuracy = train_model(seed_and_sample_unlabeled_df)\n",
    "\n",
    "        # 3.2 Initial Model + Active Learning\n",
    "        else:\n",
    "            pipelines, accuracy = active_learning(initial_pipelines, initial_seed, initial_unlabeled, sampling_size)\n",
    "\n",
    "        # 4. Report Model Accuracy\n",
    "        X_test, Y_test = test[['text_cleaned']], test[['SentimentScore']]\n",
    "\n",
    "        pl_preds = []\n",
    "        for pl in pipelines:\n",
    "            # compute the testing accuracy\n",
    "            prediction = pl.predict(pd.DataFrame(X_test))\n",
    "            pl_preds.append([pl, prediction])\n",
    "\n",
    "        #Saves all the model predictions\n",
    "        all_preds = np.array([x[1] for x in pl_preds]).transpose()\n",
    "        #Find the mode in all preds\n",
    "        prediction = [mode(i) for i in all_preds]\n",
    "        accuracy = round(accuracy_score(Y_test, prediction), round_number)\n",
    "        f1_micro = round(f1_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "        f1_macro = round(f1_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "        f1_weighted = round(f1_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "\n",
    "        precision_micro = round(precision_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "        precision_macro = round(precision_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "        precision_weighted = round(precision_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "\n",
    "        recall_micro = round(recall_score(np.array(Y_test), prediction, average='micro'), round_number)\n",
    "        recall_macro = round(recall_score(np.array(Y_test), prediction, average='macro'), round_number)\n",
    "        recall_weighted = round(recall_score(np.array(Y_test), prediction, average='weighted'), round_number)\n",
    "        \n",
    "        accuracy_lst.append(accuracy)\n",
    "        f1_micro_lst.append(f1_micro)\n",
    "        f1_macro_lst.append(f1_macro)\n",
    "        f1_weighted_lst.append(f1_weighted)\n",
    "        precision_micro_lst.append(precision_micro) \n",
    "        precision_macro_lst.append(precision_macro) \n",
    "        precision_weighted_lst.append(precision_weighted)\n",
    "        recall_micro_lst.append(recall_micro) \n",
    "        recall_macro_lst.append(recall_macro) \n",
    "        recall_weighted_lst.append(recall_weighted)\n",
    "    \n",
    "    output['accuracy'] = np.mean(accuracy_lst)\n",
    "    output['f1_micro'], output['f1_macro'], output['f1_weighted'] = np.mean(f1_micro_lst), np.mean(f1_macro_lst), np.mean(f1_weighted_lst)\n",
    "    output['precision_micro'], output['precision_macro'], output['precision_weighted'] = np.mean(precision_micro_lst), np.mean(precision_macro_lst), np.mean(precision_weighted_lst)\n",
    "    output['recall_micro'], output['recall_macro'], output['recall_weighted'] = np.mean(recall_micro_lst), np.mean(recall_macro_lst), np.mean(recall_weighted_lst)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4063903",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_method = ['random_sampling', 'active_learning']\n",
    "balanced = [True, False]\n",
    "sampling_size = [0, 200, 400, 600]\n",
    "sort_by_time = [True, False]\n",
    "partition_ratio = [[0.1, 0.45, 0.45], [0.5, 0.25, 0.25], [0.9, 0.05, 0.05]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5848d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "model_result_df = pd.DataFrame()\n",
    "index = 1\n",
    "model_name = \"SVC, KNN, Decision Tree, Random Forest, AdaBoost\"\n",
    "for tm in training_method:\n",
    "    for b in balanced:\n",
    "        for ss in sampling_size:\n",
    "            for t in sort_by_time:\n",
    "                for r in partition_ratio:\n",
    "                    print(index)\n",
    "                    model_output = trial(df, model_name, tm, b, ss, t, r)\n",
    "                    if index == 0:\n",
    "                        model_result_df = pd.DataFrame(model_output, index=index)\n",
    "                    else:\n",
    "                        model_result_df = model_result_df.append(pd.DataFrame([model_output],index=[index]))\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce332de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_names</th>\n",
       "      <th>training_method</th>\n",
       "      <th>balance</th>\n",
       "      <th>sampling_size</th>\n",
       "      <th>sort_by_time</th>\n",
       "      <th>partition_ratio</th>\n",
       "      <th>seed_size</th>\n",
       "      <th>unlabeled_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>recall_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.1, 0.45, 0.45]</td>\n",
       "      <td>39</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2382</td>\n",
       "      <td>0.2382</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>207</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.5202</td>\n",
       "      <td>0.5202</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.9, 0.05, 0.05]</td>\n",
       "      <td>375</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5404</td>\n",
       "      <td>0.5404</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>0.5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1, 0.45, 0.45]</td>\n",
       "      <td>39</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>0.4586</td>\n",
       "      <td>0.4586</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.4586</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.6052</td>\n",
       "      <td>0.4586</td>\n",
       "      <td>0.4586</td>\n",
       "      <td>0.4586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>random_sampling</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>207</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>0.5262</td>\n",
       "      <td>0.5262</td>\n",
       "      <td>0.4966</td>\n",
       "      <td>0.4966</td>\n",
       "      <td>0.5262</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.5262</td>\n",
       "      <td>0.5262</td>\n",
       "      <td>0.5262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>600</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>3682</td>\n",
       "      <td>1840</td>\n",
       "      <td>1843</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.9588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>600</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.9, 0.05, 0.05]</td>\n",
       "      <td>6627</td>\n",
       "      <td>367</td>\n",
       "      <td>371</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.9540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>600</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1, 0.45, 0.45]</td>\n",
       "      <td>735</td>\n",
       "      <td>3312</td>\n",
       "      <td>3318</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.5084</td>\n",
       "      <td>0.9314</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>0.9584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>600</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.5, 0.25, 0.25]</td>\n",
       "      <td>3682</td>\n",
       "      <td>1840</td>\n",
       "      <td>1843</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.9588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SVC, KNN, Decision Tree, Random Forest, AdaBoost</td>\n",
       "      <td>active_learning</td>\n",
       "      <td>False</td>\n",
       "      <td>600</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.9, 0.05, 0.05]</td>\n",
       "      <td>6627</td>\n",
       "      <td>367</td>\n",
       "      <td>371</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>0.9320</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.9540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model_names  training_method  \\\n",
       "1   SVC, KNN, Decision Tree, Random Forest, AdaBoost  random_sampling   \n",
       "2   SVC, KNN, Decision Tree, Random Forest, AdaBoost  random_sampling   \n",
       "3   SVC, KNN, Decision Tree, Random Forest, AdaBoost  random_sampling   \n",
       "4   SVC, KNN, Decision Tree, Random Forest, AdaBoost  random_sampling   \n",
       "5   SVC, KNN, Decision Tree, Random Forest, AdaBoost  random_sampling   \n",
       "..                                               ...              ...   \n",
       "92  SVC, KNN, Decision Tree, Random Forest, AdaBoost  active_learning   \n",
       "93  SVC, KNN, Decision Tree, Random Forest, AdaBoost  active_learning   \n",
       "94  SVC, KNN, Decision Tree, Random Forest, AdaBoost  active_learning   \n",
       "95  SVC, KNN, Decision Tree, Random Forest, AdaBoost  active_learning   \n",
       "96  SVC, KNN, Decision Tree, Random Forest, AdaBoost  active_learning   \n",
       "\n",
       "    balance  sampling_size  sort_by_time    partition_ratio  seed_size  \\\n",
       "1      True              0          True  [0.1, 0.45, 0.45]         39   \n",
       "2      True              0          True  [0.5, 0.25, 0.25]        207   \n",
       "3      True              0          True  [0.9, 0.05, 0.05]        375   \n",
       "4      True              0         False  [0.1, 0.45, 0.45]         39   \n",
       "5      True              0         False  [0.5, 0.25, 0.25]        207   \n",
       "..      ...            ...           ...                ...        ...   \n",
       "92    False            600          True  [0.5, 0.25, 0.25]       3682   \n",
       "93    False            600          True  [0.9, 0.05, 0.05]       6627   \n",
       "94    False            600         False  [0.1, 0.45, 0.45]        735   \n",
       "95    False            600         False  [0.5, 0.25, 0.25]       3682   \n",
       "96    False            600         False  [0.9, 0.05, 0.05]       6627   \n",
       "\n",
       "    unlabeled_size  test_size  accuracy  f1_micro  f1_macro  f1_weighted  \\\n",
       "1              186        192    0.2822    0.2822    0.2382       0.2382   \n",
       "2              102        108    0.5206    0.5206    0.4974       0.4974   \n",
       "3               18         24    0.5420    0.5420    0.5294       0.5294   \n",
       "4              186        192    0.4586    0.4586    0.4104       0.4104   \n",
       "5              102        108    0.5262    0.5262    0.4966       0.4966   \n",
       "..             ...        ...       ...       ...       ...          ...   \n",
       "92            1840       1843    0.9588    0.9588    0.3388       0.9388   \n",
       "93             367        371    0.9540    0.9540    0.3260       0.9320   \n",
       "94            3312       3318    0.9584    0.9584    0.3382       0.9390   \n",
       "95            1840       1843    0.9588    0.9588    0.3388       0.9388   \n",
       "96             367        371    0.9540    0.9540    0.3260       0.9320   \n",
       "\n",
       "    precision_micro  precision_macro  precision_weighted  recall_micro  \\\n",
       "1            0.2822           0.2944              0.2944        0.2822   \n",
       "2            0.5206           0.5202              0.5202        0.5206   \n",
       "3            0.5420           0.5404              0.5404        0.5420   \n",
       "4            0.4586           0.6052              0.6052        0.4586   \n",
       "5            0.5262           0.4852              0.4852        0.5262   \n",
       "..              ...              ...                 ...           ...   \n",
       "92           0.9588           0.5862              0.9364        0.9588   \n",
       "93           0.9540           0.3180              0.9100        0.9540   \n",
       "94           0.9584           0.5084              0.9314        0.9584   \n",
       "95           0.9588           0.5862              0.9364        0.9588   \n",
       "96           0.9540           0.3180              0.9100        0.9540   \n",
       "\n",
       "    recall_macro  recall_weighted  \n",
       "1         0.2822           0.2822  \n",
       "2         0.5206           0.5206  \n",
       "3         0.5420           0.5420  \n",
       "4         0.4586           0.4586  \n",
       "5         0.5262           0.5262  \n",
       "..           ...              ...  \n",
       "92        0.3394           0.9588  \n",
       "93        0.3330           0.9540  \n",
       "94        0.3396           0.9584  \n",
       "95        0.3394           0.9588  \n",
       "96        0.3330           0.9540  \n",
       "\n",
       "[96 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c583f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_df.to_csv('sentiment_committee_model_result.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d77f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d908a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5ac57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829093d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00794f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a88e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064764de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54fd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164b135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa623de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade2f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
